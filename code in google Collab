!pip install scikit-learn-intelex
!pip install daal4py
!pip install modin[ray]
!pip install tensorflow
!pip install openvino
# Import necessary libraries
import numpy as np
import modin.pandas as mpd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearnex import patch_sklearn  # Intel Extension for Scikit-learn
patch_sklearn()  # Enable Intel optimizations for sklearn
from openvino.runtime import Core  # OpenVINO Toolkit for optimization
from xgboost import XGBRegressor  # Intel-optimized XGBoost
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from sklearn.model_selection import train_test_split
import time
import os
import json  # For saving model in JSON format

# Set seed for reproducibility
np.random.seed(1234)

# Load training and test data using Modin
train_df = mpd.read_csv('/content/drive/MyDrive/INTEL_AI/PM_train.txt', sep=" ", header=None).dropna(axis=1)
test_df = mpd.read_csv('/content/drive/MyDrive/INTEL_AI/PM_test.txt', sep=" ", header=None).dropna(axis=1)
truth_df = mpd.read_csv('/content/drive/MyDrive/INTEL_AI/PM_truth.txt', sep=" ", header=None).dropna(axis=1)

# Preprocessing: Assign column names
cols_names = ['id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f's{i}' for i in range(1, 22)]
train_df.columns = cols_names
test_df.columns = cols_names
truth_df.columns = ['additional_rul']

# Add Remaining Useful Life (RUL) for training data
train_df['RUL'] = train_df.groupby('id')['cycle'].transform('max') - train_df['cycle']

# Add RUL for test data using truth_df
truth_df['id'] = truth_df.index + 1  # Align IDs in truth data
rul = test_df.groupby('id')['cycle'].max().reset_index()  # Get max cycles for each engine in test data
rul.columns = ['id', 'max_cycle']
truth_df = truth_df.merge(rul, on='id', how='left')
truth_df['RUL'] = truth_df['max_cycle'] + truth_df['additional_rul']
test_df = test_df.merge(truth_df[['id', 'RUL']], on='id', how='left')
test_df['RUL'] = test_df['RUL'] - test_df['cycle']  # Update RUL for each test sample

# Normalize sensor data using StandardScaler
features = [col for col in cols_names if col not in ['id', 'cycle', 'RUL']]
scaler = StandardScaler()
train_df[features] = scaler.fit_transform(train_df[features])
test_df[features] = scaler.transform(test_df[features])

# Prepare data for training and testing
X = train_df[features].values
y = train_df['RUL'].values
X_test = test_df[features].values
y_test = test_df['RUL'].values

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an Intel-optimized XGBoost Model
xgb_model = XGBRegressor(tree_method="hist", predictor="cpu_predictor", n_estimators=100, max_depth=6)
train_start = time.time()
xgb_model.fit(X_train, y_train)
train_time = time.time() - train_start
print(f"Training Time: {train_time:.2f} seconds")

# Predict RUL on validation and test data
val_start = time.time()
y_val_pred = xgb_model.predict(X_val)
val_time = time.time() - val_start
test_start = time.time()
y_test_pred = xgb_model.predict(X_test)
test_time = time.time() - test_start
print(f"Validation Inference Time: {val_time:.2f} seconds")
print(f"Test Inference Time: {test_time:.2f} seconds")

# Define a threshold for binary classification (e.g., w1 = 30 cycles)
w1 = 30
y_val_binary = (y_val <= w1).astype(int)  # Actual validation labels
y_val_pred_binary = (y_val_pred <= w1).astype(int)
y_test_binary = (y_test <= w1).astype(int)  # Actual test labels
y_test_pred_binary = (y_test_pred <= w1).astype(int)

# Calculate and plot Confusion Matrix for Test Data
test_confusion_matrix = confusion_matrix(y_test_binary, y_test_pred_binary)
plt.figure(figsize=(8, 6))
sns.heatmap(test_confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Failure', 'Failure'], yticklabels=['No Failure', 'Failure'])
plt.title('Confusion Matrix - Test Data')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Calculate and plot ROC Curve for Test Data
fpr, tpr, _ = roc_curve(y_test_binary, y_test_pred)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')
plt.title('ROC Curve - Test Data')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid()
plt.show()

# SHAP Feature Importance
explainer = shap.Explainer(xgb_model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test, feature_names=features)
      
# Visualize Actual vs Predicted RUL
plt.figure(figsize=(10, 6))
plt.plot(y_test[:50], label='Actual RUL')
plt.plot(y_test_pred[:50], label='Predicted RUL')
plt.legend()
plt.title('Actual vs Predicted RUL')
plt.xlabel('Samples')
plt.ylabel('RUL')
plt.grid()
plt.show()

# Save the trained XGBoost model as JSON
model_json_path = "C:/Hackathon/xgb_model.json"
xgb_model.get_booster().save_model(model_json_path)
print(f"Model saved in JSON format to {model_json_path}")

# Print Results
test_accuracy = accuracy_score(y_test_binary, y_test_pred_binary)
print(f"Test Accuracy: {test_accuracy}")
print("Test Confusion Matrix:\n", test_confusion_matrix)
